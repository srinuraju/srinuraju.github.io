<h2>Project 1 — Incremental Batch Data Pipeline on AWS</h2>

<h3>Context</h3>
<p>
  This project involved building and operating an incremental batch data pipeline on AWS to support downstream analytics
  use cases. The focus was on reliability in production, repeatable backfills, and data that consumers could trust.
</p>

<h3>What needed to be solved</h3>
<p>
  Source data arrived in batch form on a regular cadence, but analytics teams needed consistent partitions and support
  for late-arriving or updated records. Earlier approaches relied on manual reprocessing and ad-hoc fixes, which increased
  operational effort and made failures harder to diagnose.
</p>

<h3>How the pipeline was set up</h3>
<ul class="bullets">
  <li>Raw data landed in <strong>Amazon S3</strong>, partitioned by ingestion date.</li>
  <li>Transformations were implemented using <strong>PySpark</strong> on managed Spark infrastructure via <strong>AWS Glue</strong>.</li>
  <li>Curated outputs were written back to S3 in <strong>columnar Parquet</strong>, partitioned for analytics access.</li>
  <li><strong>Apache Airflow (MWAA)</strong> handled scheduling, retries, and date-range backfills.</li>
  <li><strong>AWS Glue Data Catalog</strong> managed metadata; <strong>Amazon Athena</strong> supported validation and ad-hoc analysis.</li>
</ul>
<p class="muted">
  Depending on daily loads and backfill scope, the pipeline typically processed <strong>tens to hundreds of GB per run</strong>.
</p>

<h3>Handling incremental data</h3>
<ul class="bullets">
  <li>Incremental boundaries were driven by date partitions and update timestamps.</li>
  <li>A rolling lookback window captured late-arriving records without full reprocessing.</li>
  <li>Airflow DAGs were parameterized to support controlled backfills by date range.</li>
  <li>Writes were designed to be idempotent so re-runs were safe and predictable.</li>
</ul>

<h3>Keeping data reliable</h3>
<ul class="bullets">
  <li>Schema validation for required columns and expected data types.</li>
  <li>Partition-level freshness and volume checks to catch missing or partial loads.</li>
  <li>Duplicate and key integrity checks to prevent downstream metric issues.</li>
  <li><strong>SQL-based validation queries</strong> executed through Athena for reconciliation.</li>
</ul>
<p>
  Failures blocked downstream consumption and triggered alerts, reducing the risk of silent data issues.
</p>

<h3>Operating in production</h3>
<ul class="bullets">
  <li>Task-level retries and failure handling configured in Airflow.</li>
  <li>Centralized logging and basic metrics monitored through CloudWatch.</li>
  <li>Alerts for job failures and missing or delayed partitions.</li>
  <li>Runbooks documented common failure modes and recovery steps.</li>
</ul>
<p>
  This reduced manual intervention and shortened troubleshooting time during incidents.
</p>

<h3>Performance and cost considerations</h3>
<ul class="bullets">
  <li>Controlled file sizes to reduce small-file overhead in S3.</li>
  <li>Partitioning aligned with common query access patterns in Athena.</li>
  <li>Incremental processing minimized unnecessary compute and reprocessing.</li>
</ul>

<h3>Outcome</h3>
<p>
  The pipeline produced stable, analytics-ready datasets consumed through <strong>Amazon Athena</strong> for downstream
  analysis and reporting. Standardizing file layout, partitioning, and validation reduced ad-hoc data fixes and improved
  confidence in query results for consumers.
</p>

<h3>What I’d improve next</h3>
<ul class="bullets">
  <li>Introduce dataset-level SLAs and lineage tracking.</li>
  <li>Expand anomaly detection for volume and distribution drift.</li>
  <li>Adopt open table formats (Iceberg / Delta / Hudi) with automated compaction.</li>
</ul>
